from QLearning.Model import build_model
from keras.models import save_model, load_model
import numpy as np
from collections import deque
import gym
import sys
import os
from random import sample


'''
Agent that is used in the Cartpole environment to maximize score by keeping pole up as long as possible by using
Deep Q learning.
'''


class RLAgent:

    # constructor for agent
    def __init__(self):

        # creating openai environment
        self.env = gym.make("CartPole-v0")

        # memory deque that stores the episodes of training
        self.memory = deque(maxlen=2000)

        # hyperparameters

        # how much the future reward is discounted
        self.discount_factor = .95

        # chance that a random action will be taken
        self.exploration_rate = 1.0

        # smallest chance that a random action will be taken
        self.min_exploration = 0.05

        # rate at which the exploration chance will decay
        self.exploration_decay = 0.995

        # creating untrained neural network model
        self.model = build_model()

        # number of games for training
        self.train_iterations = 3500

        # number of memories that will be taken from memory and used to train model
        self.replay_num = 32

        # number of games for testing
        self.test_iterations = 500

    '''
    Trains agent in cartpole environment.
    '''
    def train(self):

        # arrays and dequeues created for statistical purposes
        avg = np.array([])
        twenty_avg = deque(maxlen=20)
        hundred_avg = deque(maxlen=100)

        # clearing command line
        os.system("clear")

        score = 0

        # running training for previously specified number of games
        for iteration in range(self.train_iterations):

            # calculating statistics that will be displayed to user
            percent = ((iteration + 1) / self.train_iterations) * 100
            avg = np.insert(avg, 0, score)
            twenty_avg.append(score)
            hundred_avg.append(score)

            # printing statistics
            sys.stdout.write("\r|||Iteration %d/%d (%.2f%%)|||Most recent score- %d|||Total Average score- %.2f|||"
                             "Average of Last 20- %.2f|||Average of Last 100- %.2f|||Exploration Rate- %.3f|||" %
                             (iteration + 1, self.train_iterations, percent, score, avg.mean(),
                              np.array(list(twenty_avg)).mean(), np.array(list(hundred_avg)).mean(),
                              self.exploration_rate))

            observation = self.env.reset().reshape(1, 4)
            done = False

            score = 0

            # while pole has not fallen or score of 200 not achieved, run game and acquire memories
            while not done:

                score += 1

                # action, either random or generated by model
                action = self._action(observation)

                # making action and receiving new information about environment
                next_observation, reward, done, _ = self.env.step(action)
                next_observation = next_observation.reshape(1, 4)

                # remembering what happened in previous step and resulting actions
                self._remember(observation, action, reward, next_observation, done)
                observation = next_observation

            # once game is completed, rerun through memories and train model
            self._replay()

        # once training is complete, save model
        self.model.save("./QLearning/QLearningModel.h5")

    '''
    Tests agent performance in the environment.
    '''
    def test(self, modelname=''):

        scores = []

        # loading model or using model agent already has
        if modelname != '':
            self.model = load_model(modelname)

        # running through predetermined number of games
        for i in range(self.test_iterations):

            done = False
            observation = self.env.reset().reshape(1, 4)

            score = 0

            # while game is running
            while not done:

                self.env.render()

                score += 1

                # asking model to make an action that maximizes the reward given a state
                observation, reward, done, _ = self.env.step(np.argmax(self.model.predict(observation)[0]))
                observation = observation.reshape(1, 4)

            scores.append(score)

        self._print_statistics(scores)

    '''
    Remember states, actions, and rewards from the environment.
    '''
    def _remember(self, observation, action, reward, next_observation, done):

        # adds to memory
        self.memory.append([observation, action, reward, next_observation, done])

    '''
    Performs either random action or asks model to make an action based on state of hyperparameters.
    '''
    def _action(self, observation):

        # random action to be taking if still exploring
        if np.random.rand() <= self.exploration_rate:

            return np.random.randint(2)

        # asking model to make a prediction
        return np.argmax(self.model.predict(observation)[0])

    '''
    Takes memories from memory bank and trains model on them to better predict the action that maximizes the reward
    given a state. 
    '''
    def _replay(self):

        # if the amount of memories is less than the sample size to be taking for retraining, do not perform a replay
        try:

            # taking a batch from the memory bank
            batch = sample(self.memory, self.replay_num)
        except ValueError:
            return

        # decreasing chance of exploration
        if self.exploration_rate > self.min_exploration:
            self.exploration_rate *= self.exploration_decay

        # looping through the data in the batch
        for observation, action, reward, next_observation, done in batch:

            target = reward

            # if game was ended, don't map to the next reward
            if not done:
                target = reward + self.discount_factor * np.amax(self.model.predict(next_observation)[0])

            prediction = self.model.predict(observation)

            # setting the action with the associated future reward
            prediction[0][action] = target

            # training to predict closer to target
            self.model.fit(observation, prediction, verbose=0, epochs=1)

    '''
    Prints statistics after testing a model.
    '''
    def _print_statistics(self, scores):

        print("\nTotal score achieved after %d iterations- %d" % (self.test_iterations, sum(scores)))
        print("Max score- %d" % max(scores))
        print("Mean score- %.2f" % np.mean(scores))
        print("Median score - %d" % np.median(scores))